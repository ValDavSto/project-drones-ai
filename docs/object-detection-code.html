<!DOCTYPE html SYSTEM "about:legacy-compat">
<html lang="en-US" data-preset="contrast" data-primary-color="#307FFF"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="UTF-8"><meta name="robots" content="noindex"><meta name="built-on" content="2025-07-10T10:08:33.9415949"><title>Image Classification with EdgeTPU and PiCamera2 | Drone Project</title><script type="application/json" id="virtual-toc-data">[{"id":"libraries","level":0,"title":"Libraries","anchor":"#libraries"},{"id":"files-and-resources","level":0,"title":"Files and Resources","anchor":"#files-and-resources"},{"id":"model-interpreter-configuration","level":0,"title":"Model Interpreter Configuration","anchor":"#model-interpreter-configuration"},{"id":"camera-configuration","level":0,"title":"Camera Configuration","anchor":"#camera-configuration"},{"id":"main-loop","level":0,"title":"Main Loop","anchor":"#main-loop"},{"id":"graceful-shutdown","level":0,"title":"Graceful Shutdown","anchor":"#graceful-shutdown"},{"id":"notes","level":0,"title":"Notes","anchor":"#notes"}]</script><script type="application/json" id="topic-shortcuts"></script><link href="https://resources.jetbrains.com/writerside/apidoc/6.22.0-b776/app.css" rel="stylesheet"><meta name="msapplication-TileColor" content="#000000"><link rel="apple-touch-icon" sizes="180x180" href="https://jetbrains.com/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="https://jetbrains.com/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="https://jetbrains.com/favicon-16x16.png"><meta name="msapplication-TileImage" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-144x144.png"><meta name="msapplication-square70x70logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-70x70.png"><meta name="msapplication-square150x150logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-150x150.png"><meta name="msapplication-wide310x150logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-310x150.png"><meta name="msapplication-square310x310logo" content="https://resources.jetbrains.com/storage/ui/favicons/mstile-310x310.png"><meta name="image" content=""><!-- Open Graph --><meta property="og:title" content="Image Classification with EdgeTPU and PiCamera2 | Drone Project"><meta property="og:description" content=""><meta property="og:image" content=""><meta property="og:site_name" content="Drone Project Help"><meta property="og:type" content="website"><meta property="og:locale" content="en_US"><meta property="og:url" content="writerside-documentation/object-detection-code.html"><!-- End Open Graph --><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content=""><meta name="twitter:title" content="Image Classification with EdgeTPU and PiCamera2 | Drone Project"><meta name="twitter:description" content=""><meta name="twitter:creator" content=""><meta name="twitter:image:src" content=""><!-- End Twitter Card --><!-- Schema.org WebPage --><script type="application/ld+json">{
    "@context": "http://schema.org",
    "@type": "WebPage",
    "@id": "writerside-documentation/object-detection-code.html#webpage",
    "url": "writerside-documentation/object-detection-code.html",
    "name": "Image Classification with EdgeTPU and PiCamera2 | Drone Project",
    "description": "",
    "image": "",
    "inLanguage":"en-US"
}</script><!-- End Schema.org --><!-- Schema.org WebSite --><script type="application/ld+json">{
    "@type": "WebSite",
    "@id": "writerside-documentation/#website",
    "url": "writerside-documentation/",
    "name": "Drone Project Help"
}</script><!-- End Schema.org --></head><body data-id="Object-Detection-Code" data-main-title="Image Classification with EdgeTPU and PiCamera2" data-article-props="{&quot;seeAlsoStyle&quot;:&quot;links&quot;}" data-template="article" data-breadcrumbs="Object-Detection.md|Object Detection"><div class="wrapper"><main class="panel _main"><header class="panel__header"><div class="container"><h3>Drone Project  Help</h3><div class="panel-trigger"></div></div></header><section class="panel__content"><div class="container"><article class="article" data-shortcut-switcher="inactive"><h1 data-toc="Object-Detection-Code" id="Object-Detection-Code.md">Image Classification with EdgeTPU and PiCamera2</h1><p id="-dxc5bs_3">This Python script captures images from a Raspberry Pi camera using PiCamera2, classifies them in real-time using a TensorFlow Lite model optimized for the Coral EdgeTPU, and sends classification results via a custom messages module.</p><section class="chapter"><h2 id="libraries" data-toc="libraries">Libraries</h2><div class="code-block" data-lang="python">
import time
import numpy as np
import cv2
from picamera2 import Picamera2
from tflite_runtime.interpreter import Interpreter, load_delegate
import messages
</div><ul class="list _bullet" id="-dxc5bs_13"><li class="list__item" id="-dxc5bs_15"><p id="-dxc5bs_21"><span class="control" id="-dxc5bs_22">time:</span> Controls the delay between classification cycles.</p></li><li class="list__item" id="-dxc5bs_16"><p id="-dxc5bs_23"><span class="control" id="-dxc5bs_24">numpy:</span> Handles image data and numerical operations.</p></li><li class="list__item" id="-dxc5bs_17"><p id="-dxc5bs_25"><span class="control" id="-dxc5bs_26">cv2 (OpenCV):</span> Image resizing and format conversion.</p></li><li class="list__item" id="-dxc5bs_18"><p id="-dxc5bs_27"><span class="control" id="-dxc5bs_28">picamera2:</span> Interface for accessing the Raspberry Pi Camera.</p></li><li class="list__item" id="-dxc5bs_19"><p id="-dxc5bs_29"><span class="control" id="-dxc5bs_30">tflite_runtime:</span> Lightweight TensorFlow interpreter for running TFLite models.</p></li><li class="list__item" id="-dxc5bs_20"><p id="-dxc5bs_31"><span class="control" id="-dxc5bs_32">messages:</span> Custom module to send notifications/messages.</p></li></ul></section><section class="chapter"><h2 id="files-and-resources" data-toc="files-and-resources">Files and Resources</h2><div class="code-block" data-lang="python">
model_path = &quot;models/tf2_mobilenet_v3_edgetpu_1.0_224_ptq_edgetpu.tflite&quot;
label_path = &quot;models/imagenet_labels.txt&quot;
CONFIDENCE_THRESHOLD = 0.5
</div><ul class="list _bullet" id="-dxc5bs_34"><li class="list__item" id="-dxc5bs_37"><p id="-dxc5bs_40"><span class="control" id="-dxc5bs_41">model_path:</span> Path to the pre-trained quantized MobileNetV3 TFLite model.</p></li><li class="list__item" id="-dxc5bs_38"><p id="-dxc5bs_42"><span class="control" id="-dxc5bs_43">label_path:</span> Text file containing class labels.</p></li><li class="list__item" id="-dxc5bs_39"><p id="-dxc5bs_44"><span class="control" id="-dxc5bs_45">CONFIDENCE_THRESHOLD:</span> Minimum confidence level required to accept a classification.</p></li></ul><div class="code-block" data-lang="python">
labels = {}
with open(label_path, 'r') as f:
    for i, line in enumerate(f):
        labels[i] = line.strip()
</div><ul class="list _bullet" id="-dxc5bs_36"><li class="list__item" id="-dxc5bs_46"><p id="-dxc5bs_47">Loads the TFLite model and labels used for classification.</p></li></ul></section><section class="chapter"><h2 id="model-interpreter-configuration" data-toc="model-interpreter-configuration">Model Interpreter Configuration</h2><div class="code-block" data-lang="python">
edgetpu_delegate = load_delegate('libedgetpu.so.1')
interpreter = Interpreter(model_path=model_path, experimental_delegates=[edgetpu_delegate])
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
</div><ul class="list _bullet" id="-dxc5bs_49"><li class="list__item" id="-dxc5bs_51"><p id="-dxc5bs_53">Loads the model using the EdgeTPU delegate to run inference on the Coral accelerator.</p></li><li class="list__item" id="-dxc5bs_52"><p id="-dxc5bs_54">Retrieves input and output tensor metadata.</p></li></ul></section><section class="chapter"><h2 id="camera-configuration" data-toc="camera-configuration">Camera Configuration</h2><div class="code-block" data-lang="python">
picam2 = Picamera2()
picam2.configure(picam2.create_preview_configuration(main={&quot;format&quot;: &quot;RGB888&quot;, &quot;size&quot;: (224, 224)}))
picam2.start()
</div><ul class="list _bullet" id="-dxc5bs_56"><li class="list__item" id="-dxc5bs_58"><p id="-dxc5bs_59">Initializes the PiCamera with a preview configuration that matches the model&rsquo;s input size and color format (RGB888, 224&times;224 pixels).</p></li></ul></section><section class="chapter"><h2 id="main-loop" data-toc="main-loop">Main Loop</h2><div class="code-block" data-lang="python">
while True:
    ...
</div><ul class="list _bullet" id="-dxc5bs_61"><li class="list__item" id="-dxc5bs_69"><p id="-dxc5bs_70">Runs indefinitely until manually interrupted.</p></li></ul><section class="chapter"><h3 id="1-capture-image" data-toc="1-capture-image">1. Capture Image</h3><div class="code-block" data-lang="python">
frame = picam2.capture_array()
</div><ul class="list _bullet" id="-dxc5bs_72"><li class="list__item" id="-dxc5bs_73"><p id="-dxc5bs_75">Captures a single frame from the PiCamera.</p></li><li class="list__item" id="-dxc5bs_74"></li></ul></section><section class="chapter"><h3 id="2-preprocessing" data-toc="2-preprocessing">2. Preprocessing</h3><div class="code-block" data-lang="python">
img = cv2.resize(frame, (input_shape[2], input_shape[1]))
img = img.astype(np.uint8)
input_data = np.expand_dims(img, axis=0)
</div><ul class="list _bullet" id="-dxc5bs_77"><li class="list__item" id="-dxc5bs_78"><p id="-dxc5bs_79">Resizes the image to fit the model&rsquo;s input shape and adds a batch dimension.</p></li></ul></section><section class="chapter"><h3 id="3-model-inference" data-toc="3-model-inference">3. Model Inference</h3><div class="code-block" data-lang="python">
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
</div><ul class="list _bullet" id="-dxc5bs_81"><li class="list__item" id="-dxc5bs_82"><p id="-dxc5bs_83">Sends the image to the model and triggers inference.</p></li></ul></section><section class="chapter"><h3 id="4-output-extraction" data-toc="4-output-extraction">4. Output Extraction</h3><div class="code-block" data-lang="python">
output_data = interpreter.get_tensor(output_details[0]['index'])[0]
top_k = output_data.argsort()[-1:][::-1]
class_id = int(top_k[0])
confidence = output_data[class_id] / 255.0 if output_data[class_id] &gt; 1 else output_data[class_id]
</div><ul class="list _bullet" id="-dxc5bs_85"><li class="list__item" id="-dxc5bs_86"><p id="-dxc5bs_87">Retrieves the model's top prediction and calculates confidence, normalizing it if necessary.</p></li></ul></section><section class="chapter"><h3 id="5-classification-decision" data-toc="5-classification-decision">5. Classification Decision</h3><div class="code-block" data-lang="python">
if confidence &gt; CONFIDENCE_THRESHOLD:
    label = labels.get(class_id, &quot;Unknown&quot;)
    print(f&quot;Recognized: {label} with probability of {confidence:.2f}&quot;)
    messages.sendMessage(f&quot;Recognized: {label} with probability of {confidence:.2f}&quot;)
else:
    print(&quot;Nothing recognized with high confidence.&quot;)
    messages.sendMessage(&quot;Nothing recognized with high confidence.&quot;)
</div><ul class="list _bullet" id="-dxc5bs_89"><li class="list__item" id="-dxc5bs_90"><p id="-dxc5bs_92">Compares the confidence score against the threshold.</p></li><li class="list__item" id="-dxc5bs_91"><p id="-dxc5bs_93">If confident, prints and sends the result; otherwise sends a default message.</p></li></ul></section><section class="chapter"><h3 id="6-delay-between-inferences" data-toc="6-delay-between-inferences">6. Delay Between Inferences</h3><div class="code-block" data-lang="python">
time.sleep(0.5)
</div><ul class="list _bullet" id="-dxc5bs_95"><li class="list__item" id="-dxc5bs_96"><p id="-dxc5bs_97">Adds a short delay between cycles to reduce load and avoid redundant classifications.</p></li></ul></section><section class="chapter"><h3 id="7-example-output" data-toc="7-example-output">7. Example Output</h3><div class="code-block" data-lang="none">
Recognized: banana with probability of 0.87
Nothing recognized with high confidence.
</div></section></section><section class="chapter"><h2 id="graceful-shutdown" data-toc="graceful-shutdown">Graceful Shutdown</h2><div class="code-block" data-lang="python">
except KeyboardInterrupt:
    print(&quot;Finished&quot;)
</div><ul class="list _bullet" id="-dxc5bs_101"><li class="list__item" id="-dxc5bs_103"><p id="-dxc5bs_104">Terminates the program cleanly when interrupted by the user.</p></li></ul></section><section class="chapter"><h2 id="notes" data-toc="notes">Notes</h2><ul class="list _bullet" id="-dxc5bs_105"><li class="list__item" id="-dxc5bs_107"><p id="-dxc5bs_109">You can adjust CONFIDENCE_THRESHOLD to control the model's sensitivity.</p></li><li class="list__item" id="-dxc5bs_108"><p id="-dxc5bs_110">Ensure that libedgetpu.so.1 is correctly installed on your device for EdgeTPU acceleration.</p></li></ul></section><div class="last-modified">07 Juli 2025</div><div data-feedback-placeholder="true"></div><div class="navigation-links _bottom"><a href="red-object-detection-code.html" class="navigation-links__prev">Red Object Detection</a><a href="camera.html" class="navigation-links__next">Camera</a></div></article><div id="disqus_thread"></div></div></section></main></div><script src="https://resources.jetbrains.com/writerside/apidoc/6.22.0-b776/app.js"></script></body></html>